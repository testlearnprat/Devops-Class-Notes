---------------------------------------------------------------------------------------------------------------
Kubernetes
---------------------------------------------------------------------------------------------------------------
10-12-22 - class 2
---------------------------------------------------------------------------------------------------------------
Installation:

Link: https://github.com/artisantek/kubernetes-installation

kubectl  --> k8s controll line commands
kubectl get nodes --> To check all the nodes on the cluster

kubectl label node <node-name> node-role.kubernetes.io/worker1= --> To give a role to a node
kubectl label node <node-name> node-role.kubernetes.io/worker1-  --> to remove label
---------------------------------------------------------------------------------------------------------------
Pod Definition File:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
---------------------------------------------------------------------------------------------------------------
Commands:

kubectl apply -f <file>.yaml --> To apply a configuration in kubernetes
0r 
kubectl create -f <file>.yaml --> To apply a configuration in kubernetes

kubectl get pods --> To list the pods in the cluster
kubectl get pods -o wide --> to list in wide format
kubectl get po -o json/yaml/-w --> -w to watch continuos
kubectl get pods -l <label>  --> to list pods with such labels
ex: kubectl get pods -l env=dev
kubectl get pods -n <namespacename>  --> to list pods in namespace

kubectl token create --print-join-command  --> create kubeadm join token command


kubectl describe <object-kind> <name> --> To see information about a kubernetes object
kubectl delete <object-kind> <name> --> To delete a kubernetes object
example: kubectl delete pod <nameof the pod>

kubectl get deploy -->
kubectl delete deploy --> to delete controller
kubectl apply -f test.yml --> to start the deployer
kubectl get rs
kubectl scale deploy nginx-deployment --replicas=5

---------------------------------------------------------------------------------------------------------------

Manifest file fields:
apiVersions: ex: v1, apps/v1, policy/v1, scheduling.k8s.io/vi, autoscaling,storage  etc
It specifies the version of k8 api to be used for creating k8 objects. It can be v1, app/v1 etc
kubectl api-versions --> to list api versions
kubectl api-resources --api-group <api-version-name> --> to check the objects that can be created by a particular version

kind:
It specifies the type of k8 object to be created. It can be pod, service , deploy, replicaset etc
ex: pod , service, deployment etc

metadata:
It is used to set information about the objects like name,labels, namespace-under which the object will be running etc

spec: 
It consists of core information i.e the desired state of the object 

labels:
they are key value pairs attached to a k8 object which are useful in grouping and selecting the objects
ex:
labels:
  app: nginx
  os: ubuntu
kubectl get pods -l <labels> --> to display pods with a label

selectors: 
They are used to identify the k8 objects using their labels. 
There are 2 types of selectors in k8: 
1)Equity based selectors: Used to identify objects by key and then exact value. operators allowed are: =, ==, !=
ex: app = nginx , os = ubuntu
2) Set based selectors: used to identify objects by keys based on a set of values. Operators allowed are : in, notin, exists
ex: app in (nginx, tomcat, jenkins) , environment in (dev, test, prod)
kubectl get pods -l 'app in (dev, test, prod)'

-------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------

CONTROLLERS:

Replica sets/ Replication controller
They are used to create multiple instances of a single pod to achieve load balancing and high availability
If any pod gets deleted then the controller will recreate a new pod

Diff between replica set / replication controller 
Both are used to create multiple replicas of pod. The replication controller(v1) only supports equity selectors while the replica set supports both set based and equity based selector.

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80


kubectl scale rs <name>  --replicas=10  --> to change number of replicas

for set based:
matchExpressions:
(key: app, operator: in, values: nginx )

replica set uses recreate update strategy
deployment uses rolling update strategy
---------------------------------------------------------------------------------------------------------------

Deployment controller: Its the most common object used for deploying applications in kubernetes . With deployment controller we can effortleslly rollout application updates and rollback the updates without breaking the user experiance.

Deployment Controller:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80

Commands:
kubectl set image deploy <deployname> <containername:image> --record
kubectl set image deploy nginx-deploy nginx=nginx:1.14 --record --> To update the image using Deployment Controller

kubectl rollout history deploy nginx-deploy --> To check the total recorded hostory of the Controller

kubectl rollout undo deploy nginx-deploy --> To roll back to a previous version of the application
kubectl rollout undo deploy nginx-deploy --to-revision=1 --> To roll back to a particular revision number

kubectl scale deploy nginx-deploy --replicas=5 --> To scale the pods
---------------------------------------------------------------------------------------------------------------

Daemon -> Daemon set controller ensures that a pod runs on all the nodes of the cluster , if a node is added or removed from
 a cluster daemon set automatically adds or deletes the pods.
Typical use cases: 
1) Monitoring exporters: Monitoring apps like Nodeexporter, Prometheus use daemon set to monitor all the nodes in our cluster
2) Logs collection: Fluentd application collects the logs from all the nodes using daemon set 

:StatefullSet Controller: Like a deployment cntroller, statefullset also manages pods that are based on an identical container 
specification unlike deployment statefullset maintains uniqe identity for each oftheir pods, i.e the pods that are created will 
have their own state and their own volume  

use efs mount on all pods, nfs, clusterfs to replicate data in all pods as an alternative for statefull

Stateless application : applications which donot store any data in them....all pods share common storage unit in backend



Pod Phases/Pod Lifecycle: The phase of a pod  is a simple high level summary where the pod is in its lifecycle 
1) Pending --> The pod has been accepted by the k8 cluster  but one or more containers present inside the pod have not yet been started 
2)Running --> The pod has been scheduled to a node and all the containers have been created and up and running
3)Succeeded --> All the containers in the pod  have terminated in successs and will not be restarted 
4)Failed --> All the containers in the pod have terminated but atleast one container has terminated in failure i.e the container exited with non zero status
5) Unknown --> The pod goes to  unknown state if the state of the pod could not be obtained due to an error in communicating with node where pod was running 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

PROBES: Probes are the actions performed by the kubelet which help in monitering and checking the condition of the applications inside the pods.
They are defined at the container level and we can define multiple probes in the manifest file.

Probe Types: 
1) Readiness probe: with readiness probe if the conditions configured in the manifest file fail,the application or the pod are not allowed to serve external requests.
2)Liveness probe: Rather then just stopping the pods to serve the external requests like readiness probe, liveness probe incase of faiure it kills the container and 
restarts it which might have the application to get back to its initial healthy state
3)Startup probe


Configuribg probe: 
1)initialDelaySeconds [Default-0, Min-0]:  It is the number of seconds between the container start and the first probe action.
2)periodSeconds [Default-10, Min-1]: -- it is the frequency in seconds of the probe action after the initial delay time
3)timeoutSeconds [Default-1, Min-1]: It is a number of seconds the probe waits for a response from pod before assuming failure 
4)SuccessThreshold [Default-1, Min-1]: The number of consecutive positive responses needed to switch probe status to success 
5)failureThreshold [ default-1, Min-1]: - the number of consecutive negative responses needed to assume the pod has failed 

Probe Actions:
1. Shell[exec]: This probe acton is used to run shell commands inside the pods and the response is considered failure if the command exits with non zero value 
syntax:

<probe-type>
  exec:
  command:
  - command1
  - command2

Example:

apiVersion: v1
kind: Pod
metadata:
  name: alpine
  labels: 
    app: alpine
spec:
  containers:
  - name: alpine
    image: alpine
    args:
    - /bin/sh
    - -c
    - touch /home/probecheck; sleep 15; rm /home/probecheck; sleep 10000000
    livenessProbe:
      exec:
        command:
        - cat
        - /home/probecheck
      initialDelaySeconds: 2
      periodSeconds: 3
      failureThreshold: 3


2) HTTP Request [httpget]: This probe action sends an http get request to the path defines inside the probe , the http response code determines whether the probe is successfull or not.
syntax:
<probe-type>
  httpGet:
    path: <path-inside-container>
    port: <port>

apiVersion: v1
kind: Pod
metadata:
  name: liveness
  labels: 
    app: probe
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 2
      periodSeconds: 3
      failureTreshold: 3

3) TCP Port Check [tcpSocket]: --> This probe action is used to check if a port of the pod is open and if the kubelet can connect to that specific port 
syntax: 
<probe-type>
  tcpSocket:
    port: <port-number>

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    readinessProbe:
      tcpSocket:
        port: 100

Startup Probe [1.16]:
Both startup and livenessprobe have same properties i.e incase of failure they restart the container.
If both strtup and liveness probe are set, after container creation kubelet will execute startup probe first, if startup probre succeeds than liveness probe takes over further action. if startup probe fails the pod is restarted and the cycle is repeated. It is an extra layer of check for the pods

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SERVICES: In k8 pods can communicate with each other by default without any configuration by using internal ip address assigned by kubeproxy.
But these ip address are volatile in nature.
A k8 service groups a series of pods together and makes them accesseble through a common host name or ip address.

Need for k8 service:
1) since pods are short lived i.e the pod dies and a new pod comes up the ip address for that pod is most likely to change therefore we cannot relay on the pods ip address.
2)pods can be scaled up and there is no way to know the new ip address of the pod in advance.
3)we cannot use the ip address of the pod if we want to connect to the application from the outside world

Types of service:
There are 4 types of services: Clusterip, nodeport, loadbalancer and headless.

1)Cluster ip: It is the default service type in k8, and it exposes the pods internally within the cluster, it enables pods to communicatee to other pods inside the cluster through a common ip and dns.

apiVersion: v1
kind: Service
metadata:
  name: cip
spec:
  type: ClusterIP
  ports:
  - targetPort: 8080
    port: 8080
  selector:
    app: helloworld

---------------------------------------------------------------------------------------------------------------------------------------------------

NodePort: with NodePort service we can expose pods in the cluster to the outside world.

Range: 30000-32767

apiVersion: v1
kind: Service
metadata:
  name: np
spec:
  type: NodePort
  ports:
  - targetPort: 8080
    port: 8080
    nodePort: 31000
  selector:
    app: helloworld

-------------------------------------------------------------------------------------------------------------------------------------------
Load Balancer: Load balancer service will create an internal k8 service that is connected to a load balancer provided by any cloud provider. 
with load balancer service node port and and cip services are created automatically.
Load balancers are used as the ip of k8 master n worker node are not long lived
-------------------------------------------------------------------------------------------------------------------------------------
Headless service: Headless service is the same as the default cluster ip but lacks proxying(do not have constant ip) allowing us to connect to a pod directly.
Headless service will return all the ip of the pods which are associated with it.
It is used for discovering individual pods(especially IPs) which allows another service to interact directly with the Pods instead of a proxy.
That does not allocate an IP address or forward traffic.So you can do this by explicitly setting ClusterIP to “None” in the mainfest file, which means no cluster IP is allocated.
use ngnix-dp-0.hl to connect to that pod  -- dns name
When there is no need of load balancing or single-service IP addresses.We create a headless service which is used for creating a service grouping. That does not allocate an IP address 
or forward traffic.So you can do this by explicitly setting ClusterIP to “None” in the mainfest file, which means no cluster IP is allocated.

Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS 
server returns a single IP which is the service’s cluster IP. But if you don’t need the cluster IP for your service, you can set 
ClusterIP to None , then the DNS server will return the individual pod IPs instead of the service IP.Then client can connect to 
any of them.

Use Cases of Headless Service-
-Create Stateful service
-Deploying RabbitMQ to Kubernetes requires a stateful set for RabbitMQ cluster nodes.
-Deployment of Relational databases

apiVersion: v1
kind: Service
metadata:
   name: hl
spec:
  clusterIP: None
  ports:
  - targetPort: 80
    port: 80
  selector:
    app: nginx
---------------------------------------------------------------------------------------------------------------
Image: artisantek/k8s-helloworld
---------------------------------------------------------------------------------------------------------------

Manually Scheduling Pods: 
By default pod scheduling in k8 is handled by scheduler. The scheduler ensures that the right node is selected by checking the resources 
available on the worker nodes, however there are scenarios where you want your pods to end up on certain specific nodes
1)To schedule pods on nodes with speficic resources available on them.
2)To co-locate a pod from one service with a a pod from another service on the same node due to strong dependency with eachother

1)NodeSelector: It is the simplest way of scheduling the pods on to a particular node by using the node labels.

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  nodeSelector:
    spec: high
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
---------------------------------------------------------------------------------------------------------------
Commands:
kubectl get nodes --show-labels --> to display all the labels associated with node
kubectl label node <nodename> <key=value> --> to add label to a node
kubectl label node<nodename> <key>-   --> to remove a label

2) Affinity - The affinity greatly expands the node selector by adding soft and hard scheduling rules. If the soft rule is not met the scheduler can still schedule the pods on to some node.

Hard rule[requiredDuringScheduling]: Rules must be met compulsorily for a pod to be schdules on a node.
Soft Rule[Preferred during Schduling]: Scheduler will try to schedule the pods based on the preferences defined, if scheduler doesnot find any matching node , it will schedule the pod to any nodes.

IgnoredDuringExecution: If labels on a node change during run time such that the node ceizes to satisfy affinity rules the pods will still continue to run on the node.

Types:
a. nodeAffinity: like node selector we can use node affinity to schedule pods on specific nodes.

1. Hard Rule
apiVersion: v1
kind: Pod
metadata:
  name: nginx
   labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: spec
            operator: In
            values: 
            - high
            - medium
---------------------------------------------------------------------------------------------------------------
2. Soft Rule

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: spec
            operator: In
            values:
            - type
      - weight: 3
        preference:
          matchExpressions:
          - key: spec
            operator: In
            values:
            - high
---------------------------------------------------------------------------------------------------------------

Kubernetes Operators manage application logic and are part of the Kubernetes control plane. As such, they are controllers that execute loops to check 
the actual state of the cluster and the desired state, acting to reconcile them when the two states are drifting apart.

kubernetes security to restrict communication between pods -- use rbac to define which pod communicate with wch other, pod in namespaces A dont communicate 
with namespace B... create policies make previlazed container...turn on audit logging

to get central logs from pod -- using node level logging agaent , grafana dashboard , streamingsidecar container

--------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
POD AFFINITY / ANTI AFFINITY
With pod affinity and antiaffinity we can define whether a pod or should not be schedules on to a particular node based on lebels of other pods already running on that node


Deploy DB with nodeSelector:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      name: db
      labels:
        app: db
    spec:
      nodeSelector:
        spec: high
      containers:
      - name: db
        image: nginx
        ports:
        - containerPort: 80
---------------------------------------------------------------------------------------------------------------
Deploy Web app with Anti-Affinity

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      name: web
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx
        ports:
        - containerPort: 80
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - db           
            topologyKey: kubernetes.io/hostname
---------------------------------------------------------------------------------------------------------------
Deploy Redis App with Affinity:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      name: redis
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: nginx
        ports:
        - containerPort: 80
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
            topologyKey: kubernetes.io/hostname
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web           
            topologyKey: kubernetes.io/hostname
---------------------------------------------------------------------------------------------------------------

Redis and memecached  -- > cache application used for low latency along with web apps

Taints and tolerations: Affinity is a property of pods that attracts them to a set of nodes while the taints are the opposite. They allow a node to repel a set of pods
If a pod must be deployed to a tainted node tolerations are added to the pod definition file.

kubectl taint node <node-name> <key>=<value>:<taint-action> --> To taint a node
kubectl taint node <node-name> <key>=<value>:<taint-action>- To untaint a node

Taint Actions:
1)No schedule --> unless a pod has matching tolerations it wont be schedule onto the node.
2)prefernoschedule --> it is the soft version of no schedule.
3)noexecute --> k8s will immediately evict all the pods without the matching tolerations from the node. 

Pod With Tolerations:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  tolerations:
  - key: taint
    operator: Equal
    value: w1
    effect: NoSchedule

---------------------------------------------------------------------------------------------------------------------------------------------
kubectl delete deploy --all
kubectl get all
------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------

Kuberenetes Volumes:

Simple Mount:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
  - name: simple-mount
    hostPath:
      path: /tmp/simplemount/
      type: DirectoryOrCreate
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: simple-mount
 
Ephemeral volumes: These are short lived , they are tightly dependent with the lifetime of the pods and they are deleted if the pods goes down.

Persistant volumes: Persistent volumes are meant for long term storage and are independent of the pods lifecycle.
Types of persistent volumes: Static PV and Dynamic PV

In static provisioning an admin manually creates pools of persistant volumes
PV --> PVC --> mount it to a pod

PVC:  we can use the persistentt volume pools created by an administrator. Persistent volume claims are a way for a developer to request storage for the application.

Persistent Volume:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv
spec:
  reclaimPolicy: Recycle
  capacity:
    storage: 2Gi
  hostPath:
    path: /tmp/simplemount/
  storageClassName: static
  accessModes:
  - ReadWriteOnce
---------------------------------------------------------------------------------------------------------------
Persistent Volume Claim:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec:
  storageClassName: static
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
---------------------------------------------------------------------------------------------------------------
Mounting PVC to Pod

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  volumes:
  - name: persistent-volumes
    persistentVolumeClaim:
      claimName: pvc
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /home
      name: persistent-volumes
---------------------------------------------------------------------------------------------------------------


Dynamic Volumes: In dynamic volumes we configure storage classess to dynamically create persistant volumes.
The main goal of storage class is to eliminate the need for administrators to preprovision storage.
---------------------------------------------------------------------------------------------------------------------------------------
The access modes:

ReadWriteOnce -- the volume can be mounted as read-write by a single node 
ReadOnlyMany -- the volume can be mounted as read-only by many nodes 
ReadWriteMany -- the volume can be mounted as read-write by many nodes
ReadwriteOncePod -- 
---------------------------------------------------------------------------------------------------------------
Reclaim Policies:
Retain: When the PersistentVolumeClaim is deleted, the PersistentVolume will still exists, 
and the volume is considered "released". But it is not yet available for another claim because the 
previous claimant's data remains on the volume. An administrator must manually reclaim the volume.

Delete: Delete reclaim policy removes both the Persistent Volume object from Kubernetes, 
as well as the associated storage

Recycle: recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume 
and makes it available again for a new claim.
---------------------------------------------------------------------------------------------------------------
Assignments:
1. Read about Pod failures
2. Read about emptyDir Volume Mount
3. Read about init containers and side car containers

https://kukulinski.com/10-most-common-reasons-kubernetes-deployments-fail-part-1/
https://kukulinski.com/10-most-common-reasons-kubernetes-deployments-fail-part-2/

Most Common Reasons Kubernetes Deployments Fail/Pod Fail
1.ImagepullBackoff - missing image/tag or permission to pull the image
2.CrashLoopBackOff - application crashing after launch - check logs
3.RunContainerError - missing config 
4.ContainerCreating  - missing secret
5.Exceeding CPU/Memory Limits
6.Resource Quotas
7.Insufficient Cluster Resources
8.PersistentVolume fails to mount
9.Validation Errors
10.Container Image Not Updating
---------------------------------------------------------------------------------------------------------------

Sidecar containers are containers that are needed to run alongside the main container. The two containers 
share resources like pod storage and network interfaces. The sidecar containers can also share storage volumes 
with the main containers, allowing the main containers to access the data in the sidecars.

Init containers run before applications containers run in a pod, and sidecar containers run alongside application 
containers in a pod. One use for init containers is to bootstrap Appian with RDBMS/JDBC drivers not included in the 
Webapp Docker image (for example, MySQL or IBM Db2).

emptyDir are volumes that get created empty when a Pod is created. While a Pod is running its emptyDir exists. 
If a container in a Pod crashes the emptyDir content is unaffected. Deleting a Pod deletes all its emptyDirs
A Volume of type emptyDir that lasts for the life of the Pod, even if the Container terminates and restarts. 
If a container in a Pod crashes the emptyDir content is unaffected. All containers in a Pod share use of the emptyDir volume . 
Each container can independently mount the emptyDir at the same / or different path.

A volume always keeps data in /var/lib/docker/volumes, while mount points can be created wherever we want. 
If a container which is assigned a mount point is also assigned a volume then all data from the mount point is copied to the 
volume automatically, while the opposite is not true

Redhat based orchestration tool is openshift(UI based)

Docker uses c-groups to set cpu and memory limitations to containers  ---how much container can use
namespace -- how much container can see --- to isolate container
in k8 1.22 and above mismatch in cgroup betwwen docker n k8

Weavenetworks -- makesure pods have different ip and manages network -- assigns subnet

kubeproxy manages pod network internally and externally

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
Environment Variables:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    env:
    - name: ENV
      value: dev
    - name: DB
      value: postgress
--------------------------------------------------------------------------------------------------------------------------------

configmaps and secrets: they are k8 objects to store data in key value pairs, pods can then use the data as env variables or
as config files in a volume.
Config maps maps and secrets are the ways of separating the configuration data from he manifests which makes them more portable
Config maps are used to store data in terms of plain text which is open and redeable to all userswho have access to the cluster
Secrets aresecure objects to store sensitive data such as passwords keys etc which are encrypted in base64

ConfigMaps:

apiVersion: v1
kind: ConfigMap
metadata:
  name: username
data:
  user: abc

kubectl create configmap <configmap-name> --from-literal <key>=<value>
kubectl create configmap username --from-literal user=abc
---------------------------------------------------------------------------------------------------------------
Secrets:

apiVersion: v1
kind: Secret
metadata:
  name: password
type: Opaque
data:
  password: test

kubectl create secret generic <secret-name> --from-literal <key>=<value>
kubectl create secret generic password --from-literal password=test

echo "abc" | base64
echo "value" | base64 --decode
---------------------------------------------------------------------------------------------------------------
Mounting ComfigMaps and Secrets as ENV Variables:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    env:
    - name: ENV
      value: dev
    - name: DB
      value: postgress
    - name: USER
      valueFrom:
        configMapKeyRef:
          name: username
          key: user
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: password
          key: password
---------------------------------------------------------------------------------------------------------------
Namespace:

Kubernetes namespace allows us to partition our cluster into virtual subdivisions. 
We can create a namespace for different teams, projects etc. 
Objects in different namespaces are invisible to each other which means there won't be any conflicts.

Kubernetes has four initial namespaces:
• default: This namespace is used for creating objects when a namespace is not specified.
• kube-system: This namespace is used by Kubernetes system to create and run its components.
• kube-public: The objects inside this namespace are readable by all users including those 
               who are not authenticated to the ApiServer.
• Kube-node-lease: This namespace is a new addition to Kubernetes and it has resources 
                   which provide tools to monitor objects associated with each node.
---------------------------------------------------------------------------------------------------------------
apiVersion: v1
kind: Namespace
metadata:
  name: dev

kubectl create ns <namespace-name> --> To create a namespace
kubectl delete ns <namespace-name> --> To delete a namespace
---------------------------------------------------------------------------------------------------------------
Creating objects inside a namespace:

1. Through Manifest Files:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: dev
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
---------------------------------------------------------------------------------------------------------------
2. Through kubectl command

kubectl apply -f <.yaml-file> -n dev --> To create a K8S object within a namespace
---------------------------------------------------------------------------------------------------------------
Assignment:
Mount ConfigMap and Secret as Volumes  --  https://www.youtube.com/watch?v=_Sqy6vi6fWo
---------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------

Resource Quotas: Namespaces do not enforce limitations or quotas automatically.For this purpose
we can use k8s quotas to specify limits for around 15 k8s objects

Resource Quotas:

apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq
spec:
  hard:
    pods: 1
    configmaps: 2 
    requests.cpu: 0.5
    limits.cpu: 1
    requests.memory: 256Mi
    limits.memory: 500Mi
---------------------------------------------------------------------------------------------------------------
Pod with Limits:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    resources:
      requests:
        cpu: 0.1
        memory: 64Mi
      limits:
        cpu: 0.25
        memory: 128Mi
---------------------------------------------------------------------------------------------------------------

Securing Kubernetes:

Context:
A context is a group of access parameters to enable secure connection to a specific cluster’s API server. 
Each context contains a Kubernetes cluster, a user, and a namespace..
---------------------------------------------------------------------------------------------------------------
Types of Users in Kubernetes: 
User Account: We can create users and groups who can connect to the Kubernetes API server. 
kubernetes- admin is the default user

Service-Account:
Service accounts are used to give access to processes inside pods to interact with the
Kubernetes API. They can also be used by applications outside the cluster.

For example, Prometheus monitoring tool which is used to monitor the cluster can be given the access using 
Service-Account.
---------------------------------------------------------------------------------------------------------------
Creating Service Account:

Step1: Create a Service Account
kubectl create sa test

Step2: Get the secret token name of the service Account
kubectl describe sa test

Step3: Get the token value of the secret
kubectl describe secret <token-name>

Step4: Assign the token value to a Variable
TOKEN="<token-value-from-step3>"

Step5: Assign the credential to the SA
kubectl config set-credentials test --token=$TOKEN

Step6: Creating a Context for the Service Account
kubectl config set-context con --cluster=kubernetes --user=test
---------------------------------------------------------------------------------------------------------------
Creating User account: create ssl certificate using openssl genrsa -out name


Commands:

kubectl config get-contexts --> To display available contexts in config
kubectl config use-context con --> To switch to a context
---------------------------------------------------------------------------------------------------------------
RBAC:

RBAC or Role-Based Access Control is an approach in Kubernetes used to add constraints for users, groups, 
and applications to access Kubernetes resources. RBAC basically adds security to the Kubernetes cluster, 
and we can apply it for a specific namespace or to the total cluster.
It was introduced in version 1.8 and uses rbac.authorization.k8s.io API group. 3 important concepts in RBAC.

• Subject: Subject is the entity that needs access. It could be user or group or a service account
• Resources: Resource is the K8s object that a subject wants to access. It could be pods, deployments, services etc
• Verbs: Verbs are the actions that a subject can do on resources. It could be the list, delete, create, watch etc
---------------------------------------------------------------------------------------------------------------
Role & Cluster Role:
Role and ClusterRole contains set of rules to access & modify Kubernetes resources. 
Difference between them is Role works in a particular namespace while ClusterRole is cluster wide. 
Basically, we use Role If we want to define permissions inside a namespace and use ClusterRole for cluster wide.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cr
rules:
- apiGroups: ["*"]
  resources: ["pods", "secrets"]
  verbs: ["list", "get"]

kubectl create clusterrole cr --resources=pods --resource=secrets --verb=get --verb=list
---------------------------------------------------------------------------------------------------------------
Cluster Role Binding and Role Bindings:

Rolebinding binds the Role to a Subject to access the Resources within a namespace while 
ClusterRoleBinding binds the ClusterRole to a Subject to access the resources cluster-wide.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb
subjects:
- kind: ServiceAccount
  name: test
  namespace: default
roleRef:
  kind: ClusterRole
  name: cr
  apiGroup: rbac.authorization.k8s.io

kubectl create clusterrolebinding crb --serviceaccount=default:test --clusterrole=cr
---------------------------------------------------------------------------------------------------------------
kubectl auth can-i get pods --> To check access

kubectl edit pods podname -n namespacename --- without configuration

kubectl describe quota rq -n test
kubectl describe clusterrole cr

kubectl config view  --> to see the k8s server
---------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------

EKS Cluster [Windows]: 

Step 1: Install AWS ClI
https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html

Step 2: Open cmd.exe as Administrator

Step 3: Install Chocolatey
https://docs.chocolatey.org/en-us/choco/setup

Step 4: Install eksctl
choco install -y eksctl

Step 5: Install kubectl
choco install kubernetes-cli

Step 6: Create Cluster
eksctl create cluster \
 --name my-cluster \
 --version 1.23 \
 --with-oidc \
 --nodegroup-name worker \
 --region ap-south-1 \
 --node-type t2.medium \
 --managed \
 --ssh-access \
 --ssh-public-key oct-2022 
 
Setting Kube Config File:
aws eks --region ap-south-1 update-kubeconfig --name my-cluster

Cleanup:
eksctl delete cluster --name my-cluster --region ap-south-1
----------------------------------------------------------------------------------------------------------
Kubernetes Dashboard [UI]:

Step1: Install Kubernetes Metric Server[eks]
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

Step2: Install Kubernetes Dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

Alternatively [https://github.com/kubernetes/dashboard]

Step3: Run command line proxy
kubectl proxy

Step4: Access the Dashboard
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Note: The UI can only be accessed from the machine where the command is executed

Cleanup:
kubectl delete ns kubernetes-dashboard
----------------------------------------------------------------------------------------------------------
Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 
Traffic routing is controlled by rules defined on the Ingress resource.

For the Ingress resource to work, the cluster should have an ingress controller running. 
The famous ingress controllers are Nginx, ALB and GCE

Deploy Ingress Controller [https://kubernetes.github.io/ingress-nginx/deploy/]:

Step1: Install Helm
https://helm.sh/docs/intro/install/

Step2: Deploy Ingress Controller
helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress --create-namespace

Repo Link: https://github.com/artisantek/kubernetes-ingress


apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: hello
            port: 
              number: 80





Cleanup:
kubectl delete ns ingress
----------------------------------------------------------------------------------------------------------
Deploy EBS CSI Driver [Dynamic Volumes]:

Step1: Find the roles attached to the Worker Nodes:
kubectl -n kube-system describe configmap aws-auth

Step2: Attach IAM Policy "AmazonEBSCSIDriverPolicy" to the role

Step3: Deploy CSI Driver
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"

Example:

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: dynamic
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2 
  fsType: ext4

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec:
  storageClassName: dynamic
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
--------------------------------------------------------------------------------
--------------------------

Some of the Use cases for Init Containers can:

Cause a pod to wait for another Kubernetes resource to be created before finishing startup.
Perform sensitive startup steps securely outside of app containers.
Populate data into a shared volume at startup.
Communicate with another service at startup.
Init containers can contain utilities or custom code for the setup that are not present in an app image.
They can be given access to Secrets that app containers cannot access.
Clone a Git repository into a Volume
It can be used to wait for a service to start that is to be used by the main app
An init container is a good candidate for delaying the application initialization until one or more dependencies are available.

sidecar containers   --  https://blog.knoldus.com/sidecar-container-vs-init-container-in-kubernetes/#use-case-of-side-car-container
It used by log shippers, log watchers, monitoring agents

The main container is an Nginx container that stores its logs on a volume mounted on /var/log/nginx. Mounting a volume at that location prevents Nginx from outputting its log data to the standard output and forces it to write them to access.log and error.log files.

logs - https://sematext.com/guides/kubernetes-logging/#:~:text=Within%20a%20Kubernetes%20system%2C%20we,(or%20system%20component)%20logs.